---
title: "Inferential Statistics: Probability & Distributions - 1"
output: html_notebook
---

So far we have discussed about descriptive statistics - summarizing data and plotting it. But in order gain the power of making inferences, we will be strating with inferential statistics.

#### Pre-requisite: Probability

##### Difference between probability and statistics**
Probability theory is a branch of mathematics that tells you how often different kinds of events will happen. For eg. What are the chances of a fair coin coming up heads 10 times in a row? or What are the chances that I’ll win the lottery?

In each case the “truth of the world” is known. We know that the coin is fair, so there’s a 50% chance that any individual coin flip will come up heads. We know that the lottery follows specific rules. The critical point is that probabilistic questions start with a known model of the world, and we use that model to do some calculations. *[Chapter 9, Navarro D.]*

**A short note on Models**

A model is a simplified representation of a system. For example, the map of a city represents a city in a simplified fashion. A map providing as much detail as the original city would not only be impossible to construct, it would also be pointless. Humans build models, such as maps and statistical models, to make their lives simpler. *[Chapter 3, Winter B.]*
- - - -

But even though we know the models like `P(heads) = 0.5`, we do not know the data (Whetehr heads will come 10 times or 3 times). However, for statistics, it is the opposite. We have the data and we want to infer the truth about the world. For eg., If my friend flips a coin 10 times and gets 10 heads, are they playing a trick on me? or If the lottery commissioner’s spouse wins the lottery, how likely is it that the lottery was rigged?

We want to figure out which is the true model of the world. Is it *P(heads) = 0.5* or is it *P(heads) $\ne$ 0.5*?

##### What is probability really?

**The frequentist view**

![Frequentist_graph](Fig4.png)

According to the frequentist view, flip a fair coin over and over again, and as N grows large (approaches infinity, denoted N Ñ 8), the proportion of heads will converge to 50%.

 *Advantages*
 -  It is objective: the probability of an event is necessarily grounded in the world.
 -  It is unambiguous: any two people watching the same sequence of events unfold, trying to calculate the probability of an event, must inevitably come up with the same answer.

But it all depends on infinite flips of coin. Do infinities really exist in the physical universe? What about the probability for a single non-repeatable event like the chances of rain on 21 September 2021?

**The Bayesian view**

Bayesian view is subjectivist view. The most common way of thinking about subjective probability is to define the probability of an event as the degree of belief that an intelligent and rational agent assigns to that truth of that event. But how to operationalize this 'degree of belief'? 

One way is to use 'rational gambling'. So a “subjective probability” will be operationalized in terms of what bets you're willing to accept.

 *Advantage*
 - You don’t need to be limited to those events that are repeatable.
 
 *Disadvantage*
 - Can’t be purely objective – specifying a probability requires us to specify an entity that has the relevant degree of belief. This entity might be a human, an alien, a robot, or even a statistician, but there has to be an **intelligent agent** out there that believes in things. 


In short, frequentist view is sometimes considered to be too narrow (forbids lots of things that that we want to assign probabilities to) while the Bayesian view is sometimes thought to be too broad (allows too many differences between observers).

##### Definitions

Refer to the example described in *Section 9.3.1, Navarro D.* for the following content.

**Elementary event:** Every time we make an observation (e.g., every time I put on a pair of pants), then the outcome will be one and only one of these events.

**Sample space:** The set of all possible events (e.g., the wardrobe)

**Probability:** Numbers between 0 and 1.

For an event X, the probability of that event P(X) is a number that lies between 0 and 1. The bigger the value of P(X), the more likely the event is to occur.

If P(X) = 0, it means the event X is impossible (i.e., I never wear those pants). On the other hand, if P(X)= 1 it means that event X is certain to occur (i.e., I always wear those pants).

**Law of total probability:** The probabilities of the elementary events need to add up to 1

#### Distributions

Let's take a look at this and see what is a distribution. 

```{r}
pants <- data.frame(
   type = c("Blue jeans","Grey jeans","Black jeans","Black suit","Blue tracksuit"),
   label = c("X1", "X2", "X3", "X4", "X5"),
   probability = c(0.5,0.3,0.1,0,0.1))

pants
```
Probability distribution is simply the probabilities of these different events above. Each of the events has a probability that lies between 0 and 1, and if we add up the probability of all events, they sum to 1.

```{r}
#Try plotting a bar graph of all the probabilities above
```
Let's think about what happens in case of non-elementary events. E.g. An event E where either “blue jeans” or “black jeans” or “grey jeans" has occurred. 
Then what will be the probability of event E.

P(E) = P(X1) + P(X2) + P(X3)

If any of these elementary events occurs, then E is also said to have occurred. Similarly, there are other rules satisfying probabilities:

![Probability_rules](Fig5.png)

##### Binomial Distribution

*Refer to section 9.4.1, Navarro D., for the detailed example*

Some basic terminology - We’ll let `N` denote the number of dice rolls in our experiment; which is often referred to as the `size parameter` of our binomial distribution. We’ll also use `θ` to refer to the the probability that a single die comes up skull, a quantity that is usually called the `success probability` of the binomial. Finally, we’ll use `X` to refer to the results of our experiment, namely the number of skulls I get when I roll the dice. Since the actual value of X is due to chance, we refer to it as a `random variable`.

`X ~ Binomial(θ, N)` denotes X is generated randomly from a binomial distribution with parameters θ and N.

4 ~ Binomial(1/6, 20)

5 ~ Binomial(1/2, 10)

Let's generate a binomial distribution in R:

```{r}
dbinom( x = 4, size = 10, prob = 1/2)
```


The above command calculates the probability of getting x = 4 skulls, from an experiment of size = 20 trials, in which the probability of getting a skull on any one trial is prob = 1/6.

What if the dice is replaced by a coin in the above example? How will the probability change? 

```{r}
#Try finding the probability for N = 20 and N=100 trials for a fair coin flip.
```
There are different functions in R for different distributions as well as different ones for finding different quantity of interest.

If we want to find the probability of obtaining an outcome smaller than or equal to quantile q, then we can directly use `pbinom`.

```{r}
#Find the probability of rolling 0 skulls or 1 skull or 2 skulls or 3 skulls or 4 skulls
pbinom( q= 4, size = 10, prob = 1/2)

#Practice - Find probability of getting 0-5 heads in 50 trials of coin flip
```
In other words, value of 4 is actually the 76.9th percentile of this binomial distribution.

Now let’s say we want to calculate the 75th percentile of the binomial distribution.

```{r}
qbinom( p = 0.376, size = 10, prob = 1/2 )

#Practice - Find the 40th percentile
```

We've found different quantities. What if we want to simulate the above experiments. We specify how many times R should “simulate” the experiment using the n argument, and it will generate random outcomes from the binomial distribution using the `rbinom` function.

```{r}
z <- rbinom( n = 10000, size = 10, prob = 1/2 )
#Let's also plot this and see how it looks
hist(z, breaks=25, col = 'steelblue')
```
#Try plotting the distributions in above examples and vary the size, trial number and probability to generate different plots.
```{r}
library(ggplot2)
df<- data.frame(z)
# Basic histogram with custom binwidth
ggplot(df, aes(x=z)) + geom_histogram(aes(y = ..density..), binwidth=1, color="darkblue", fill="lightblue")
```

```{r}
dbinom(x = 43, size = 100, prob = 1/2)
```
```{r}
pbinom(q = 17, size = 20, prob = 1/2)
```
```{r}
qbinom(p = 0.975, size = 10, prob = 1/2)
```
```{r}
qbinom(p = 0.025, size = 10, prob = 1/2)
```

```{r}
binom.test( x=6, n=10, p=1/2 )
```


```{r}
z <- rbinom( n = 1000000, size = 100, prob = 0.9 )

library(ggplot2)
df<- data.frame(z)
# Basic histogram with custom binwidth
ggplot(df, aes(x=z)) + geom_histogram(aes(y = ..density..), binwidth=1, color="darkblue", fill="lightblue")
```
```{r}
library(ggplot2)
library("ggpubr")
```


```{r}
num <- 10000
z_null <- rbinom( n = num, size = 100, prob = 0.5 )
z_alt <- rbinom( n = num, size = 100, prob = 0.6 )
df1 <- data.frame(z_null,z_alt)
# Basic histogram with custom binwidth
p1 <- ggplot(df1, aes(x=z_null)) + geom_histogram(aes(y = ..density..), binwidth=1, color="lightblue", fill="lightblue")
p2 <- ggplot(df1, aes(x=z_alt)) + geom_histogram(aes(y = ..density..), binwidth=1, color="lightgreen", fill="lightgreen")

figure <- ggarrange(p1, p2,
                    labels = c("Null", "Alt"))
figure
```


```{r}
library(pwr)
pwr.p.test(h  = ES.h(p1 = 0.7, p2 = 0.5), sig.level = 0.05, power = 0.8)
```
```{r}
effsize <- ES.h(p1 = 0.7, p2 = 0.5)
effsize
```







All these different functions *d, p, q, n* are also applicable to other distributions. E.g. *dnorm, pnorm, qnorm, rnorm* for Normal distribution. 


##### Normal Distribution

Most frequently encountered distribution.
Eg: heights of all students in the class, marks obtained in exams, etc

Basically, whenever you have accumulation of data at the center, fewer extreme values and a near symmetric spread, you should recall the normal distribution.


- `dnorm()` - For probability density
- `pnorm()` - For cumulative probability
- `qnorm()` - For quantile of
- `rnorm()` - For random number generation


mean = 0; sd = 1 -> standard normal distribution
```{r}
histogram_normal_distribution
```

```{r}
normal_distribution <- rnorm(10000, mean = 75, sd = 1) 
histogram_normal_distribution <- hist(normal_distribution)
plot(histogram_normal_distribution$mids,histogram_normal_distribution$density, type="l", col="blue", lwd=1)


```

Note: Normal distribution is sometimes referred to as the bell curve or Gaussian distribution

The notation for a normal distribution is: X ∼ Normal(μ,σ) 

```{r}
no_test <- rnorm(10000, mean = 165, sd = 20)

```



dnorm tells you the probability of getting a particular outcome
```{r}
dnorm(x=165, mean=165, sd=20)

```
Cumulative normal distribution
```{r}
pnorm(q = 175, mean = 165, sd = 20)
```

```{r}
qnorm(0.75 ,mean = 165 , sd = 20)
```

*Checking for normality using the Shapiro-Wilk Test*
```{r}
norm <- rnorm(1000, mean = 0, sd = 1) 
shapiro.test(norm)



```


```{r}
binom <- rbinom(100, 10, 1/2)
shapiro.test(binom)
```



##### Other useful distributions

Some other distributions you may encounter include:
*1) t distribution*

Looks like the normal distribution but has heavier tails. 
Used when data looks like a normal distribution but the mean and SD are unknown.

Use the following functions to visualize the t distribution: 
dt(), pt(), qt() and rt()
```{r}
histogram_t_distribution

```

```{r}
t_distribution <- rt(10000, 15)
histogram_t_distribution <- hist(t_distribution)
plot(histogram_t_distribution$mids,histogram_t_distribution$density, type="l", col="blue", lwd=3)
```
```{r}

```


```{r}
```




*2) Chi square (χ2) distribution*

All positive and heavily skewed to the left.  
Used when data represents sum of squares of a normally distributed variables.

Use the following functions to visualize the chi sq distribution: 
dchisq(), pchisq(), qchisq(), rchisq().
```{r}
norm1 <- rnorm(10000, mean = 100, sd = 5)
norm2 <- rnorm(10000, mean = 200, sd = 7)
chisqdist <- norm1^2 + norm2^2

#hist(chisqdist)
histogram_chisq_distribution <- hist(chisqdist)
plot(histogram_chisq_distribution$mids,histogram_chisq_distribution$density, type="l", col="blue", lwd=3)
```
```{r}
#norm1+norm2
```


```{r}
#norm1^2+norm2^2
```

```{r}

norm1 <- rnorm(10000, mean = 10, sd = 5)
norm2 <- rnorm(10000, mean = 20, sd = 7)
chisqdist <- norm1^2 + norm2^2

chisq_distribution <- rchisq(100000, 10)
histogram_chisq_distribution <- hist(chisq_distribution)
plot(histogram_chisq_distribution$mids,histogram_chisq_distribution$density, type="l", col="blue", lwd=3)
```




3) F distribution

This one looks a bit like the chi square distribution. But this distribution comes into picture when 
one compares two chi sq distributions.

Use the following functions to visualize the chi sq distribution: 
df(), pf(), qf() and rf()

```{r}
f_distribution <- rf(10000, 100, 100)
histogram_f_distribution <- hist(f_distribution)
plot(histogram_f_distribution$mids,histogram_f_distribution$density, type="l", col="blue", lwd=3)
```

```{r}
x <- rnorm(10000, mean = 75, sd = 10)
```

```{r}
mean(x)
sd(x)
```
```{r}
y <- rt(10000,50)
 hist(x)
  hist(y)
```

The End

Reference - *Chapter 9, Navarro D.*
